{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Настройка импортов\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import plotly.express as px\n",
    "import logging\n",
    "import optuna\n",
    "from functools import partial\n",
    "import os\n",
    "import zipfile\n",
    "import warnings\n",
    "import re\n",
    "from llama_cpp import Llama"
   ],
   "id": "62fa09300035a88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"The default value of `min_cluster_size` has changed from 5 to 10. This will affect the number of clusters produced.\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The default value of `min_samples` has changed from None to `min_cluster_size`. This will affect the number of clusters produced.\")\n",
    "\n",
    "# Настройка стандартного сообщения лога\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Загрузка SpaCy модели\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    logging.info(\"SpaCy model 'en_core_web_lg' successfully loaded.\") #\n",
    "except OSError:\n",
    "    logging.error(\"SpaCy model 'en_core_web_lg' not found. Please install it with: python -m spacy download en_core_web_lg\")\n",
    "    exit()"
   ],
   "id": "51de6331cadd6229",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Функция загрузки входных данных из файла\n",
    "def process_texts_from_file(file_path: str, nlp_model):\n",
    "    logging.info(f\"Starting preprocessing of text requirements from file: {file_path}...\") #\n",
    "    requirements = []\n",
    "    processed_texts = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f: #\n",
    "            for line in f:\n",
    "                line = line.strip() # Удаление пробелов в начале/конце строки\n",
    "                if line:\n",
    "                    requirements.append(line) #\n",
    "                    processed_texts.append(preprocess_text(line, nlp_model))\n",
    "        logging.info(f\"Preprocessing completed. Processed {len(requirements)} requirements.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found at path: {file_path}\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading or processing file: {e}\")\n",
    "        return [], []\n",
    "    return requirements, processed_texts"
   ],
   "id": "813a231060f9d2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Функция пред обработки входных данных\n",
    "def preprocess_text(text: str, nlp_model) -> str:\n",
    "    stopwords = set(nlp_model.Defaults.stop_words)\n",
    "    doc = nlp_model(text.lower())\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Удаление стоп слов, пунктуации и пробелов и не алфавитных символов\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop and token.text not in stopwords and token.is_alpha: #\n",
    "            if re.search(r'[a-zA-Z]', token.lemma_):\n",
    "                tokens.append(token.lemma_)\n",
    "    return \" \".join(tokens)"
   ],
   "id": "1cab854ffeb0f155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Блок загрузки исходных данных\n",
    "data_path = \"tz_erp.md\"\n",
    "original_requirements, processed_requirements = process_texts_from_file(data_path, nlp_model=nlp)\n",
    "\n",
    "if not original_requirements:\n",
    "    logging.error(\"No data to process. Exiting program.\") #\n",
    "    exit()\n",
    "df_requirements = pd.DataFrame({\n",
    "    'Requirement': original_requirements,\n",
    "    'Processed_Requirement': processed_requirements\n",
    "})\n",
    "logging.info(f\"Loaded and preprocessed {len(df_requirements)} requirements.\") #"
   ],
   "id": "5645557fe0df801c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Функция генерации эмбеддингов\n",
    "def generate_embeddings(texts: list, model_name: str = 'thenlper/gte-large'):\n",
    "    logging.info(f\"Loading SentenceTransformer model: {model_name}...\") #\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name) # Загрузка пред обученной модели, отвечающей за генерацию эмбеддингов\n",
    "        logging.info(\"SentenceTransformer model successfully loaded.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading SentenceTransformer model: {e}\")\n",
    "        raise\n",
    "    logging.info(\"Starting embedding generation...\")\n",
    "    # Генерация эмбеддингов из обработанных входных данных\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    logging.info(f\"Embedding generation completed. Dimensionality: {embeddings.shape}\") #\n",
    "    return embeddings"
   ],
   "id": "ea6f4f2a5a615292",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Блок с вызовом функции генерации эмбеддиногов\n",
    "document_embeddings = generate_embeddings(df_requirements['Processed_Requirement'].tolist())"
   ],
   "id": "812185d2cad32f4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Функция оптимизации гиперпараметров моделей с использованием фреймворка Optuna\n",
    "def objective(trial, documents, embeddings, nlp_model):\n",
    "    # Гиперпараметры UMAP\n",
    "    # Количество соседей для построения графа локальных связей.\n",
    "    # Меньшие значения делают модель чувствительной к локальной структуре,\n",
    "    # большие — к глобальной.\n",
    "    n_neighbors = trial.suggest_int('umap_n_neighbors', 5, 20)\n",
    "    # Размерность выходного пространства (число компонентов после уменьшения размерности).\n",
    "    # Меньшие значения подходят для визуализации, большие сохраняют больше информации.\n",
    "    n_components = trial.suggest_int('umap_n_components', 2, 7)\n",
    "    # Минимальное расстояние между точками в результирующем пространстве.\n",
    "    # Меньшие значения создают плотные кластеры, большие раздвигают их.\n",
    "    min_dist = trial.suggest_float('umap_min_dist', 0.0, 0.3)\n",
    "    # Гиперпараметры HDBSCAN\n",
    "    # Минимальный размер кластера. Точки, не входящие в кластеры такого размера,\n",
    "    # считаются шумом.\n",
    "    min_cluster_size = trial.suggest_int('hdbscan_min_cluster_size', 5, 15)\n",
    "    # Минимальное количество соседей для формирования плотного региона (core points).\n",
    "    # Должно быть <= min_cluster_size.\n",
    "    min_samples = trial.suggest_int('hdbscan_min_samples', 1, min_cluster_size - 1) # min_samples <= min_cluster_size\n",
    "    if min_samples >= min_cluster_size:\n",
    "        min_samples = min_cluster_size - 1\n",
    "        if min_samples < 1: # Fallback for very small min_cluster_size\n",
    "            min_samples = 1\n",
    "\n",
    "    # Максимальное расстояние между точками в одном кластере.\n",
    "    # Большие значения объединяют близкие кластеры\n",
    "    cluster_selection_epsilon = trial.suggest_float('hdbscan_cluster_selection_epsilon', 0.0, 1.0)\n",
    "\n",
    "    # Гиперпараметров CountVectorizer\n",
    "    # Минимальная частота слова (в долях от общего числа документов),\n",
    "    # при которой оно считается слишком распространенным и исключается из словаря.\n",
    "    max_df = trial.suggest_float('vectorizer_max_df', 0.8, 1.0)\n",
    "    # Максимальная длина n-грамм (например, униграммы, биграммы, триграммы).\n",
    "    # Большие значения учитывают длинные комбинации слов.\n",
    "    ngram_range_end = trial.suggest_int('vectorizer_ngram_range_end', 1, 4) #\n",
    "\n",
    "    # Создание экземпляров моделей с предложенными гиперпараметрами\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=n_neighbors,  # Чувствительность к локальной/глобальной структуре\n",
    "        n_components=n_components,  # Размерность выходного пространства\n",
    "        min_dist=min_dist,  # Плотность кластеров\n",
    "        metric='cosine',\n",
    "        random_state=42,\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,  # Минимальный размер кластера\n",
    "        min_samples=min_samples,  # Плотность регионов\n",
    "        metric='euclidean',\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,  # Расстояние между точками\n",
    "        prediction_data=True\n",
    "    )\n",
    "\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=list(nlp_model.Defaults.stop_words),  # Исключение стоп-слов\n",
    "        min_df=1,  # Минимальная частота слова\n",
    "        max_df=max_df,  # Максимальная частота слова\n",
    "        ngram_range=(1, ngram_range_end)  # Учет n-грамм\n",
    "    )\n",
    "\n",
    "    # Инициализация BERTopic с выбранными моделями\n",
    "    topic_model_opt = BERTopic(\n",
    "        language=\"multilingual\",\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        top_n_words=10,\n",
    "        calculate_probabilities=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Обучение модели и получение тем/вероятностей\n",
    "        topics, probabilities = topic_model_opt.fit_transform(documents, embeddings)\n",
    "\n",
    "        labels = np.array(topics)\n",
    "        unique_labels = np.unique(labels)\n",
    "        num_topics = len(unique_labels[unique_labels != -1])\n",
    "        # Если кластеров недостаточно для осмысленной оценки\n",
    "        if num_topics < 2:\n",
    "            logging.warning(f\"Trial {trial.number}: Only {num_topics} non-noise topics found. Returning a poor score.\")\n",
    "            return -1.0\n",
    "        # Создаем булеву маску для исключения шумовых точек (топик -1)\n",
    "        # Метрики качества кластеризации (Silhouette, DB, CH) не предназначены для оценки шума,\n",
    "        # поэтому расчет производится только на кластеризованных данных.\n",
    "        non_noise_indices = (labels != -1)\n",
    "\n",
    "        # Проверка: если все точки являются шумом или нет кластеризованных точек\n",
    "        if not np.any(non_noise_indices):\n",
    "            logging.warning(f\"Trial {trial.number}: All points are noise. Returning a poor score.\")\n",
    "            return -1.0\n",
    "\n",
    "        # Фильтруем эмбеддинги и метки, оставляя только те, что относятся к кластерам (не к шуму)\n",
    "        filtered_embeddings = embeddings[non_noise_indices]\n",
    "        filtered_labels = labels[non_noise_indices] # Используем labels вместо topics для консистентности\n",
    "\n",
    "        if len(np.unique(filtered_labels)) < 2:\n",
    "            logging.warning(f\"Trial {trial.number}: Less than 2 unique non-noise topics after filtering. Returning a poor score.\")\n",
    "            return -1.0\n",
    "\n",
    "        # Вычисление метрик качества кластеризации\n",
    "        s_score = silhouette_score(filtered_embeddings, filtered_labels)\n",
    "        db_score = davies_bouldin_score(filtered_embeddings, filtered_labels)\n",
    "        ch_score = calinski_harabasz_score(filtered_embeddings, filtered_labels)\n",
    "\n",
    "        # Целевая функция: максимизация силуэта и минимизация Davies-Bouldin\n",
    "        objective_score = s_score - np.log1p(db_score) + np.log1p(ch_score)\n",
    "        logging.info(f\"Trial {trial.number}: Silhouette={s_score:.4f}, Davies-Bouldin={db_score:.4f}, Calinski-Harabasz={ch_score:.4f}, Score={objective_score:.4f}\")\n",
    "        return objective_score\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Trial {trial.number}: Error during training: {e}\")\n",
    "        score = -1.0 # Штраф за ошибку\n",
    "    logging.info(f\"Trial {trial.number}: UMAP(n_neighbors={n_neighbors}, n_components={n_components}, min_dist={min_dist}) | HDBSCAN(min_cluster_size={min_cluster_size}, min_samples={min_samples}, epsilon={cluster_selection_epsilon}) | Vectorizer(min_df={1}, max_df={max_df}, ngram={ngram_range_end}) -> Silhouette Score: {score:.4f}\")\n",
    "    return score"
   ],
   "id": "472b988e752dc966",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Функция, инициализирующая оптимизацию гиперпараметров моделей с заданым количеством этапов\n",
    "def optimize_bertopic_hyperparameters(documents: list, embeddings: np.ndarray, nlp_model, n_trials: int = 50):\n",
    "    logging.info(f\"Starting BERTopic hyperparameter optimization with Optuna (n_trials={n_trials})...\")\n",
    "    objective_with_data = partial(objective, documents=documents, embeddings=embeddings, nlp_model=nlp_model)\n",
    "    study = optuna.create_study(direction='maximize', study_name='BERTopic_Optimization')\n",
    "    study.optimize(objective_with_data, n_trials=n_trials, show_progress_bar=True)\n",
    "    logging.info(f\"Optimization completed. Best parameters: {study.best_params}\")\n",
    "    logging.info(f\"Best Silhouette Score: {study.best_value:.4f}\")\n",
    "    return study.best_params"
   ],
   "id": "381f36c6d57d2517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Получение оптимальных параметров\n",
    "best_params = optimize_bertopic_hyperparameters(df_requirements['Processed_Requirement'].tolist(), document_embeddings, nlp_model=nlp, n_trials=150)"
   ],
   "id": "5ee008c81a5e8d41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Функция обучения финальной модели BERTopic\n",
    "def train_bertopic_model(documents: list, embeddings: np.ndarray, best_params: dict, nlp_model):\n",
    "    logging.info(\"\\nInitializing final BERTopic model with optimized parameters...\")\n",
    "\n",
    "    # Инициализация UMAP-модели с оптимизированными гиперпараметрами\n",
    "    final_umap_model = UMAP(\n",
    "        n_neighbors=best_params['umap_n_neighbors'],  # Количество соседей для построения графа локальных связей.\n",
    "                                                       # Меньшие значения делают модель чувствительной к локальной структуре,\n",
    "                                                       # большие — к глобальной.\n",
    "        n_components=best_params['umap_n_components'],  # Размерность выходного пространства (число компонентов после уменьшения размерности).\n",
    "                                                         # Меньшие значения подходят для визуализации, большие сохраняют больше информации.\n",
    "        min_dist=best_params['umap_min_dist'],  # Минимальное расстояние между точками в результирующем пространстве.\n",
    "                                                 # Меньшие значения создают плотные кластеры, большие раздвигают их.\n",
    "        metric='cosine',  # Метрика для измерения расстояния между точками (косинусное расстояние).\n",
    "        random_state=42  # Фиксация случайного состояния для воспроизводимости результатов.\n",
    "    )\n",
    "\n",
    "    # Инициализация HDBSCAN-модели с оптимизированными гиперпараметрами\n",
    "    final_hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=best_params['hdbscan_min_cluster_size'],  # Минимальный размер кластера.\n",
    "                                                                     # Точки, не входящие в кластеры такого размера, считаются шумом.\n",
    "        min_samples=best_params['hdbscan_min_samples'],  # Минимальное количество соседей для формирования плотного региона (core points).\n",
    "                                                           # Должно быть <= min_cluster_size.\n",
    "        metric='euclidean',  # Метрика для измерения расстояния между точками (евклидово расстояние).\n",
    "        cluster_selection_epsilon=best_params['hdbscan_cluster_selection_epsilon'],  # Максимальное расстояние между точками в одном кластере.\n",
    "                                                                                        # Большие значения объединяют близкие кластеры.\n",
    "        prediction_data=True  # Сохранение данных для предсказания новых точек.\n",
    "    )\n",
    "\n",
    "    # Инициализация CountVectorizer с оптимизированными гиперпараметрами\n",
    "    final_vectorizer_model = CountVectorizer(\n",
    "        stop_words=list(nlp_model.Defaults.stop_words),  # Исключение стоп-слов (например, \"the\", \"and\").\n",
    "        min_df=1,  # Минимальная частота слова (в долях от общего числа документов).\n",
    "                     # Слова, встречающиеся реже, исключаются из словаря.\n",
    "        max_df=best_params['vectorizer_max_df'],  # Максимальная частота слова (в долях от общего числа документов).\n",
    "                                                   # Слишком распространенные слова исключаются из словаря.\n",
    "        ngram_range=(1, best_params['vectorizer_ngram_range_end'])  # Учет n-грамм (например, униграммы, биграммы, триграммы).\n",
    "                                                                      # Большие значения учитывают длинные комбинации слов.\n",
    "    )\n",
    "\n",
    "    # Инициализация финальной модели BERTopic с оптимизированными моделями\n",
    "    final_topic_model = BERTopic(\n",
    "        language=\"multilingual\",  # Поддержка мультиязычных текстов.\n",
    "        umap_model=final_umap_model,  # Модель UMAP для уменьшения размерности данных.\n",
    "        hdbscan_model=final_hdbscan_model,  # Модель HDBSCAN для кластеризации данных.\n",
    "        vectorizer_model=final_vectorizer_model,  # Модель CountVectorizer для преобразования текста в числовые векторы.\n",
    "        top_n_words=10,  # Количество топовых слов для описания каждой темы.\n",
    "        calculate_probabilities=True  # Расчет вероятностей принадлежности текстов к темам.\n",
    "    )\n",
    "\n",
    "    # Обучение финальной модели BERTopic на документах и эмбеддингах\n",
    "    topics, probabilities = final_topic_model.fit_transform(documents, embeddings)\n",
    "    logging.info(f\"Final BERTopic model training completed. Found topics: {len(final_topic_model.get_topics()) - 1}\")\n",
    "    # -1 для исключения шума (топик -1).\n",
    "\n",
    "    return final_topic_model, topics, probabilities"
   ],
   "id": "c25e2a4bd690cf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Блок получения экземпляра обученной на оптимизированных гиперпараметрах модели\n",
    "final_topic_model, topics, probabilities = train_bertopic_model(df_requirements['Processed_Requirement'].tolist(), document_embeddings, best_params, nlp)\n",
    "\n",
    "# Добавление результатов в DataFrame\n",
    "df_results = df_requirements.copy()\n",
    "df_results['Embeddings'] = document_embeddings.tolist()\n",
    "df_results['Topic'] = topics\n",
    "df_results['Topic_Probability'] = [p.max() if p is not None and len(p) > 0 else 0 for p in probabilities]\n",
    "df_results"
   ],
   "id": "7d4481f110eb8467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Расчет финальных метрик качества разбиения\n",
    "def evaluate_clustering(embeddings: np.ndarray, labels: np.ndarray):\n",
    "    metrics = {}\n",
    "    logging.info(\"\\nStarting clustering quality evaluation for the final model...\")\n",
    "\n",
    "    # Удаление всех данных, помеченных как шум\n",
    "    non_noise_indices = labels != -1\n",
    "    filtered_embeddings = embeddings[non_noise_indices]\n",
    "    filtered_labels = labels[non_noise_indices]\n",
    "\n",
    "    if len(set(filtered_labels)) < 2:\n",
    "        logging.warning(\"Insufficient clusters (less than 2, excluding noise) for calculating clustering quality metrics. Skipping metrics.\") #\n",
    "        metrics['Silhouette Score'] = np.nan\n",
    "        metrics['Davies-Bouldin Index'] = np.nan\n",
    "        metrics['Calinski-Harabasz Index'] = np.nan\n",
    "        return metrics\n",
    "\n",
    "    try:\n",
    "        # Расчет коэффициента силуэта, значения от -1 до 1.\n",
    "        metrics['Silhouette Score'] = silhouette_score(filtered_embeddings, filtered_labels)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating Silhouette Score: {e}\")\n",
    "        metrics['Silhouette Score'] = np.nan\n",
    "\n",
    "    try:\n",
    "        # Расчет коэффициента Дэвиса-Боулдина, чем ниже - тем лучше. Минимальное значение - 0\n",
    "        metrics['Davies-Bouldin Index'] = davies_bouldin_score(filtered_embeddings, filtered_labels)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating Davies-Bouldin Index: {e}\")\n",
    "        metrics['Davies-Bouldin Index'] = np.nan\n",
    "\n",
    "    try:\n",
    "        # Расчет коэффициента Калински-Харабза: Чем выше - тем лучше, минимальное значение 0\n",
    "        metrics['Calinski-Harabasz Index'] = calinski_harabasz_score(filtered_embeddings, filtered_labels)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating Calinski-Harabasz Index: {e}\")\n",
    "        metrics['Calinski-Harabasz Index'] = np.nan\n",
    "\n",
    "    logging.info(\"Clustering evaluation completed.\")\n",
    "    return metrics"
   ],
   "id": "110d70f6acd58734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Блок получения резульатов финальной кластеризации\n",
    "clustering_metrics = evaluate_clustering(document_embeddings, np.array(topics))\n",
    "logging.info(\"Clustering metrics for the final model:\")\n",
    "for metric, value in clustering_metrics.items():\n",
    "    logging.info(f\"- {metric}: {value:.4f}\" if isinstance(value, float) else f\"- {metric}: {value}\")"
   ],
   "id": "d6614d2da80655b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Функция визуализации полученных результатов\n",
    "def visualize_clusters(df_results: pd.DataFrame, umap_model: UMAP, title: str = \"Requirement Clusters Visualization\", dim: int = 2):\n",
    "    # Будет построено 2 графика с отображнием на них получившихся кластеров, в 2D и в 3D\n",
    "    logging.info(f\"Starting {dim}D cluster visualization...\")\n",
    "    if 'Embeddings' not in df_results.columns or 'Topic' not in df_results.columns:\n",
    "        logging.error(\"DataFrame for visualization must contain 'Embeddings' and 'Topic' columns.\")\n",
    "        return\n",
    "    embeddings_to_project = np.array(df_results['Embeddings'].tolist())\n",
    "    if umap_model is None:\n",
    "        logging.error(\"UMAP model not provided for visualization.\")\n",
    "        return\n",
    "    if dim == 2:\n",
    "        if umap_model.n_components != 2:\n",
    "            logging.warning(\"UMAP model initialized not for 2D. Recreating UMAP for 2D visualization.\")\n",
    "            umap_model_2d = UMAP(n_neighbors=umap_model.n_neighbors, n_components=2, min_dist=umap_model.min_dist, metric=umap_model.metric, random_state=umap_model.random_state)\n",
    "            reduced_embeddings = umap_model_2d.fit_transform(embeddings_to_project)\n",
    "        else:\n",
    "            reduced_embeddings = umap_model.fit_transform(embeddings_to_project)\n",
    "        df_results['UMAP_X'] = reduced_embeddings[:, 0]\n",
    "        df_results['UMAP_Y'] = reduced_embeddings[:, 1]\n",
    "        fig = px.scatter(df_results,x='UMAP_X',y='UMAP_Y',color='Topic',hover_data=['Requirement'],title=title,labels={'UMAP_X': 'UMAP Component 1', 'UMAP_Y': 'UMAP Component 2'})\n",
    "    elif dim == 3:\n",
    "        if umap_model.n_components != 3:\n",
    "            logging.warning(\"UMAP model initialized not for 3D. Recreating UMAP for 3D visualization.\")\n",
    "            umap_model_3d = UMAP(n_neighbors=umap_model.n_neighbors, n_components=3, min_dist=umap_model.min_dist, metric=umap_model.metric, random_state=umap_model.random_state)\n",
    "            reduced_embeddings = umap_model_3d.fit_transform(embeddings_to_project)\n",
    "        else:\n",
    "            reduced_embeddings = umap_model.fit_transform(embeddings_to_project)\n",
    "        df_results['UMAP_X'] = reduced_embeddings[:, 0]\n",
    "        df_results['UMAP_Y'] = reduced_embeddings[:, 1]\n",
    "        df_results['UMAP_Z'] = reduced_embeddings[:, 2]\n",
    "\n",
    "        fig = px.scatter_3d(df_results,x='UMAP_X',y='UMAP_Y',z='UMAP_Z',color='Topic',hover_data=['Requirement'],title=title,labels={'UMAP_X': 'UMAP Component 1','UMAP_Y': 'UMAP Component 2','UMAP_Z': 'UMAP Component 3' })\n",
    "    else:\n",
    "        logging.error(\"Unsupported dimension for visualization. Only 2D and 3D are supported.\")\n",
    "        return\n",
    "    fig.show()\n",
    "    logging.info(f\"{dim}D Visualization completed.\")"
   ],
   "id": "e4328cd75d2131ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Блок вызова построения 2D и 3D графиков\n",
    "visualize_clusters(df_results, final_topic_model.umap_model, title=\"BERTopic: Requirement Clusters Visualization (2D Optimized)\", dim=2)\n",
    "visualize_clusters(df_results, final_topic_model.umap_model, title=\"BERTopic: Requirement Clusters Visualization (3D Optimized)\", dim=3)"
   ],
   "id": "94e809d089769152",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    llm = Llama.from_pretrained(\n",
    "\t    repo_id=\"MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF\",\n",
    "\t    filename=\"Mistral-7B-Instruct-v0.3.IQ1_M.gguf\",\n",
    "        n_gpu_layers=-1, # Использует все слои на GPU (Apple Metal)\n",
    "        n_ctx=4096,      # Контекстное окно (можно увеличить, если нужно)\n",
    "        chat_format=\"mistral-instruct\", # Важно для корректного форматирования промптов\n",
    "        verbose=False    # Отключить подробный вывод llama.cpp в консоль\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Ошибка при загрузке модели: {e}\")"
   ],
   "id": "35d23616c9d6a42f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# 1. Анализ количества уникальных кластеров\n",
    "unique_topics = df_results['Topic'].unique()\n",
    "num_unique_topics = len(unique_topics)\n",
    "print(f\"Общее количество уникальных кластеров (топиков): {num_unique_topics}\")\n",
    "print(f\"Список уникальных топиков: {unique_topics}\")"
   ],
   "id": "f5f90e5e0b25af86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Группировка всех имеющихся кластеров в отдельные наборы данных\n",
    "grouped_requirements = {}\n",
    "for topic_id in unique_topics:\n",
    "    # Исключаем топик -1, который обычно относится к шуму или неклассифицированным документам\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    requirements_in_topic = df_results[df_results['Topic'] == topic_id]['Processed_Requirement'].tolist()\n",
    "    grouped_requirements[topic_id] = requirements_in_topic\n",
    "    print(f\"\\nТопик {topic_id} содержит {len(requirements_in_topic)} требований.\")\n",
    "    print(f\"Примеры требований в топике {topic_id}: {requirements_in_topic[:3]}...\")"
   ],
   "id": "57a0456afe03066c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Функция, генерирующая подробное описание кластера на основе списка требований с помощью LLM.\n",
    "def generate_system_description(requirements):\n",
    "    requirements_str = \"\\n\".join([f\"- {req}\" for req in requirements])\n",
    "    prompt = \\\n",
    "f\"\"\"\n",
    "Master prompt: Developing a detailed description of a software system based on functional requirements\n",
    "Can you create a comprehensive and accurate description of a software system by following my detailed instructions? I expect you to generate a high-quality result suitable for use at an advanced level. If you are ready for this challenge in prompt engineering, get to work.\n",
    "Goal: Based on the functional requirements provided, generate a complete, detailed, and structured description of the software system suitable for use at all stages of the development lifecycle — from conceptualization to deployment.\n",
    "Required output format:\n",
    "Your output should be extremely clear, concise, and organized, using the following structure:\n",
    "1.  System name: A short but comprehensive name that accurately reflects the essence and main purpose of the system. It should be easy to remember and intuitive.\n",
    "2.  Detailed description of the system:\n",
    "A detailed but concise overview of the system. Answer the following questions:\n",
    "What is the main purpose of the system? (The problem it solves or the value it creates).\n",
    "Who is the target audience/users?\n",
    "What are its key components and how do they interact? (A high-level overview of the architecture, without unnecessary technical details).\n",
    "What are its main functions?\n",
    "What are its expected benefits or outcomes?\n",
    "In what environment will it operate? (E.g., web, desktop, mobile, cloud).\n",
    "Use a coherent prose style to ensure smooth reading and deep understanding.\n",
    "3.  Key functionalities:\n",
    "A clear, bulleted list of all the main functionalities of the system that directly follow from the functional requirements provided.\n",
    "Each item should be:\n",
    "Specific: Describe one action or one capability.\n",
    "Measurable: Allow determination of whether the feature is implemented.\n",
    "Achievable: Realistic to implement within the system.\n",
    "Relevant: Directly related to the system's objectives.\n",
    "Time-bound: (Although this is not always applicable to individual features, keep the project context in mind).\n",
    "Avoid redundancy and general phrases.\n",
    "4.  Suggested high-level project directory structure:\n",
    "Present a logical, intuitive, and scalable project directory structure. This structure should be general enough to apply to most modern software projects, but detailed enough to serve as a practical guide. Include, but don't limit yourself to, the following examples, if applicable and appropriate:\n",
    "src/ (source code)\n",
    "tests/ (tests: unit, integration, e2e)\n",
    "docs/ (documentation: API, user guides, architecture)\n",
    "Use a hierarchical representation (e.g., with indentation) for clarity.\n",
    "Input: {requirements_str}\n",
    "Processing instructions:\n",
    "Analysis: Carefully analyze each functional requirement. Identify the main entities, interactions, business rules, and user scenarios.\n",
    "Synthesis: Combine disparate requirements into a single, logically coherent description of the system.\n",
    "Detailing: If necessary, supplement general requirements with more specific details to ensure completeness of description without deviating from the essence of the original requirements.\n",
    "Clarity and accuracy: Avoid ambiguity. Use precise technical terminology where appropriate, but maintain readability for a broad audience.\n",
    "Scalability: Present the system in such a way that it allows for future expansion and changes.\n",
    "Example of expected tone and style: Professional, authoritative, accurate, concise, results-oriented.\n",
    "Confidence in the result: I expect you to demonstrate outstanding prompt engineering skills by delivering a result that goes beyond simple answers and sets a new standard of quality for this type of task. Are you up for the challenge?\n",
    "\n",
    "Translated with DeepL.com (free version)\n",
    "\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm.create_chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            stop=[\"<|im_end|>\"], # Токен остановки для Mistral (может быть и другими, но это часто используется)\n",
    "    )\n",
    "    generated_text = response['choices'][0]['message']['content'].strip()\n",
    "    return generated_text"
   ],
   "id": "c1b43ba08a8ce668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Создание папки внутри проекта для сохранения файлов с описанием кластеров\n",
    "output_dir = \"cluster_descriptions\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ],
   "id": "b877f586acc76c55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Цикл, отвечающий за генерацию описания для кластера и запись его в отдельный файл\n",
    "all_output_files = []\n",
    "for topic_id, requirements in grouped_requirements.items():\n",
    "    print(f\"\\nОбработка топика {topic_id}...\")\n",
    "    if not requirements:\n",
    "        print(f\"В топике {topic_id} нет требований. Пропускаем.\")\n",
    "        continue\n",
    "    # Генерация описания кластера\n",
    "    system_description_en = generate_system_description(requirements)\n",
    "    print(f\"Сгенерировано описание для топика {topic_id}.\")\n",
    "    topic_name_row = topic_id\n",
    "    output_filename = os.path.join(output_dir, f\"system_description_{topic_id}.md\")\n",
    "    all_output_files.append(output_filename)\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# System Description for Topic: {topic_id} ({topic_name_row})\\n\\n\")\n",
    "        f.write(system_description_en)\n",
    "        f.write(\"\\n---\\n\")"
   ],
   "id": "7af636f23f7ff0f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Блок, объединяющий полученные файлы с описанием в один zip архив\n",
    "zip_filename = \"cluster_system_descriptions.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in all_output_files:\n",
    "        zipf.write(file_path, os.path.basename(file_path))\n",
    "print(f\"\\nВсе описания систем объединены в архив: {zip_filename}\")\n",
    "print(f\"Архив сохранен в корневом каталоге проекта.\")\n"
   ],
   "id": "45a75a65b61a5a26",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
